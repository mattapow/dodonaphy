\documentclass[11pt, twocolumn]{article}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{epstopdf}
\usepackage{hyperref}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\newtheorem{theorem}{Theorem}
\newcommand{\be}{\begin{equation*}}
\newcommand{\ee}{\end{equation*}}
\newcommand{\bee}{\begin{eqnarray*}}
\newcommand{\eee}{\end{eqnarray*}}

\title{Variational Phylogenetic Inference in Hyperbolic Space}

\begin{document}
\maketitle

\section{Abstract}
\textbf{
Hyperbolic spaces allow quality embeddings of nested data structures, such as trees.
Recent efforts to embed phylogenetic trees using distance-based optimisation demonstrate promising results for embedding single trees.
However, expressing uncertainty in phylogenetics involves a probability distribution over a super-exponential number of trees.
Here, using low dimensional hyperbolic tree embeddings, we explore the Bayesian posterior distribution of trees in a continuous manner.
First, we empirically demonstrate that a posterior surface can be well approximated with tree embeddings using an MCMC in the embedding space.
We also evaluate the potential of variational inference in the embedding space.
}

\section{Introduction}
Bayesian phylogenetics has struggled when provided with many taxa.
The super-exponential number of combinations of tree topologies is largely responsible for this;
even a modest one hundred taxa provides more tree topologies than the number of atoms in the universe, clearly marginalising over these topologies is intractable.
Markov Chain Monte Carlo (MCMC) has become a routine way forwards in this high dimensional space.
However, its computational performance is lacklustre compared to online viral outbreaks, calling for faster methods.

One possibility is a variational approximation to the posterior, which minimises the divergence between a chosen approximating function and the posterior.
This technique avoids the computational burden of computing the marginal likelihood of the data by instead optimising a lower bound for the evidence (ELBO).
Furthermore, it is possible to encode multiple tree topologies in a continuous manner using embeddings.
Embedding trees into Riemannian manifolds can enable the efficiencies gains from gradient based optimisation, even as tree topologies change.

A successful embedding requires an isometry between distances in trees and distances in the embedding space. 
Previous work has shown that ultrametric trees always have an isometric embedding in Euclidean space~\cite{pavoine2005measuring}.
Furthermore, there is an isometric embedding in Euclidean space for any rooted phylogenetic tree by taking the square-root of distances between leaves on the tree~\cite{devienne2011euclidean}.
However, Euclidean embeddings generally have a minimum embedding dimension~\cite{deza1997geometry}, trading one high-dimensional problem for another.
Spurred on by quality embeddings of nested data structures (such as trees) in low dimensions~\cite{sala2018representation}, several recent works turn to hyperbolic space for embeddings~\cite{chami2020trees, wilson2021learning, matsumoto2020novel}.
However, none of these works consider the problem of Bayesian inference in hyperbolic space.

In this work, we embed points onto a sphere $S^{d-1}$ in hyperbolic space $\mathbb{H}^d$, with each point corrresponding to a genetic sequence.
Initially points are embedded according to their pairwise genetic distance using an approximate strain minimisation algorithm.
Each point is equipped with a variational distribution on the sphere.
To sample a tree, we draw one point from each distribution and form the neighbour joining tree from their pairwise distance in hyperbolic space.
We then optimise these distributions by maximising the ELBO.



\section{Hyperbolic Embeddings}
The Poincaré ball $\mathbb{P}^{d} = \{x\in \mathbb{R}^{d} \,:\, ||x||<1\}$ models hyperbolic space $\mathbb{H}^{d}$ using the metric:
\begin{equation*}\label{eq:poin_dist}
d(x,y) = \textnormal{arccosh}\Big(1 + 2 \frac{||x-y||_{2}^{2}}{ (1-||x||_{2}^{2}) (1-||y||_{2}^{2})}\Big),
\end{equation*}
where $||x||_{2}$ is the $l^{2}$-norm in $\mathbb{R}^{d}$.

We form continuous embeddings of trees using one point in the Poincare ball $\bold{x} \in \mathbb{P}^{d}$ for each node in the tree.
For an unrooted tree with $S$ taxa, there will be $m=2S - 2$ nodes locations $\bold{X} \in(\mathbb{P}^{d})^{m}$, where $\bold{X} = \{\bold{x}_{i}: i=1, ..., m\}$.
Embedded nodes are connected to form a minimum spanning tree (MST) protocol that ensures internal nodes have three neighbours and tip nodes have one neighbour.
Both the branch lengths and tree topology may freely change as nodes move.
Once a tree $T(\bold{X})$ is formed, its prior probability $p(T)$ and the likelihood of a sequence aligmnent p(D\big|T) under a given model and data $D$ may be easily determined.

This method is quite distinct from cost functions that are based on pair-wise distances. For example Chami's variant on Dasgupta's cost, the log-a-like used by Wilson or see refs in Chami. Here, nodes placements do not directly contribute to the cost function (tree posterior), they only impact how a tree is formed.


\begin{theorem} \label{thm:accessible}
There exists an isometric embedding of a finite metric space with === into hyperbolic space.
\end{theorem}
Practically, we find a slight improvement of the variational approximation when the embedding is restricted to a sphere.
We hypothesise that ultrametric trees can be embedded into hyperbolic space with their leaves lying on a sphere.

\section{Sampling Trees in Hyperbolic Space}
It is convenient to sample in Euclidean space using common distributions, so we use a homeomorphism between $\mathbb{P}^{d}$ and $\mathbb{R}^{d}$ to embed points.
The embedding function $g: \mathbb{R}^{d} \to \mathbb{P}^{d}$ is:
\be
g(\bold{x}) = \dfrac{\bold{x}}{1+||\bold{x}||_{2}}
\ee
with gradient
\be
\frac{\partial g(\bold{x})}{\partial \bold{x}} = 
\dfrac{1}{1+||\bold{x}||_{2}} \Big( I - \dfrac{\bold{x} \otimes \bold{x}}{(1+||\bold{x}||_{2})||\bold{x}||_{2}} \Big).
\ee

Leaves are restricted to a sphere by normalising their radius to a single value.
Proposals are still multivariate normals in $\mathbb{R}^{d}$, but then the radius is  normalised to radius of the first leaf.

We must show that the Markov Chains are ergodic, that is, as $m \to \infty$ the distribution $p(\bold{X}_{m}|\bold{X}_{0})$ converges to the posterior distribution regardless of the choice of $p(\bold{X}_{0})$.
If the chain is not ergodic, then the choice of proposal partitions the state space into subsets which cannot be reached from each other.
A point set $\bold{X}_{m} \in \mathbb{R}^{d}$ can be reached via our normal distribution proposal, however it remains to be seen if this embedding spans all trees.
We must know theorem~\ref{thm:accessible}.
% https://www.robots.ox.ac.uk/~fwood/teaching/C19_hilary_2014_2015/mcmc.pdf

\section{MCMC}
Since each set of node embeddings corresponds to a tree which has a well defined likelihood and prior probability, MCMC can proceed with the standard Metropolis-Hastings algorithm.
The set of nodes are initialised to locations $\bold{X}_{0}$.
For each node $\bold{x}_{i} \in \bold{X}_{0}$ a new location is proposed and accepted or rejected according to the Metropolis-Hastings algorithm, giving the next iteration $\bold{X}_{1}$.

% \begin{figure}[htbp]
% \begin{center}
%     \includegraphics[width=\linewidth, height=\textheight]{fig/t17_trace}
% \end{center}
% \end{figure}

\begin{figure}[htbp] \label{fig:map_dim}
\begin{center}
    \includegraphics*[width=\linewidth]{fig/map_dim}
\end{center}
\caption{Improvement of the MAP estimate on a tree with 17 taxa with increasing dimension.}
\end{figure}


\begin{figure}[htbp] \label{fig:splits}
\begin{center}
    \includegraphics*[width=\linewidth]{fig/t17_splits}
\end{center}
\caption{Comparison of splits on a tree with 17 taxa.}
\end{figure}



\section{Variational Inference}
Intro to VI===

\begin{figure}[htbp] \label{fig:elbo_trace}
    \begin{center}
        \includegraphics*[width=\linewidth]{fig/elbo_trace_t6_simple_nj_lr2_k10_d5}
    \end{center}
    \caption{ELBO tree on a tree with 6 taxa. With 17 taxa, the ELBO either decreases or stays roughly constant.}
    \end{figure}

\section{Comparison to MrBayes}
A posterior was approximated for a 17 taxa set using Dodonaphy's MCMC and its VI before being compared to MrBayes.
A tree was simulated using a birth (rate $2$) death (rate $.5$) model and a sequence alignment was generated from this tree under the JC69 model of genetic evolution.
The pairwise patristic distances were computed between the tips on the simulated tree.
The tips were initialised in the Poincare ball using hydra.
Then the internal nodes were randomly placed in $\mathbb{P}^{d}$ with uniform directional and radius from a scaled Beta distribution $ r \sim s \times \text{Beta}(a=2, \beta=5)$ using scales $s \in [0, 2*\min(d(0, \bold{x}_{i})]$, where $i$ only includes the tip nodes.
Internal node locations were sampled $10^{4}$ times and the initialisation with the highest tree likelihood was selected.




\clearpage
\section{Thoughts}
\subsection{Embedding}
\begin{itemize}
\item Want a MST with given tips and degree constrains:
Given tips see \href{https://cs. stackexchange.com/questions/40131/spanning-tree-with-chosen-leaves?rq=1}{this} stackexchange question.
But degree constrained is harder. 
\href{https://cs.stackexchange.com/questions/22775/minimal-spanning-tree-with-degree-constraint}{This} algorithm looks at only constraining the .

\begin{verbatim}
For n-2 iterations:
    Find the pair of edges i and j \
        neighbouring a single internal\
        vertex that minimises the cost\
        C(i)+C(j)
    Add edges i and j to the tree
\end{verbatim}
\item I tried without the Matsumuto adjustment and the likelihood was a bit worse.
\end{itemize}

\subsection{MCMC}
\begin{itemize}
\item What type of tree rearrangements occur when topology changes?
\item Posterior surfance is smooth if topology doesn't change.
\item Remove isometric component of new vector. Want rotations and translations component
\item I tried using log-a-like function instead of likelihood \cite{wilson2021learning} (leaves constrained to a sphere). It gave terrible trees on a six taxa dataset - likelihood $\sim -8000$ (not $-2600$) and bad splits. I tried weighting the edges inversely with their length (shorter edges are weighted more) and the result was about the same.
\item Try on more taxa 20, 100, 200 using hpc
\item How easy is it to add taxa? Might only need pair-wise distance to a subset of other taxa.
\end{itemize}

\subsection{VI}
\begin{itemize}
\item Pytorch's optimisers don't converge for non-convex problems.
\item Posterior ``surface'' is non-convex and not continuous...
\item What if we learn the curvature?
\end{itemize}

\subsection{Full rank}
Intuitively, nodes that are close together should be a bit correlated. However, it doesn't seem to improve things much. In the off-diagonals in the covariance matrix are initialised to zero, the ELBO gets much higher faster compared to if the off-diagonal terms have a non-zero covariance. That said, I haven't run simulations long enough to be sure, only $1000$ epochs with a small learning rate of $0.01$.

\subsection{Distance-based}
Could we adopt an approach like Wilson and Chami, where only the distributions of the embedded points are optimised based on their pair-wise distances.
Only then do we infer a tree.
The advantage of this is that the cost function is differentiable and more in line with what other people in the ML community do.
However, this isn't actually modelling the Bayesian Posterior, just a proxy for it.


%\newpage
%\section{Variational Inference}
%In VI, the posterior surface (here a set of surfaces --- one for each embedded node $i$) is approximated by a variational distribution $q_{i}(z_{i})$, each parameterised by parameters $z_{i}$.
%When the data $D$ is fixed, minimising the KL divergence to the posterior $\text{KL}({\bf q}(\phi)||p(T\big|D))$ is equivalent to maximising the evidence lower bound (ELBO):
%\be
%\text{ELBO}(q) = \mathbb{E}_{q}[\log(p(z,D))] - \mathbb{E}_{q}[\log(q(z))].
%\ee
%Working with the ELBO is computationally advantageous since it does not depend on the intractable marginal distribution of the data $p(D)$, which is involves integrate over all tree topologies and all branch lengths.
%We consider two mean-field variational distributions for the node locations in $\mathbb{P}^{d}$: a logit-Normal distribution and a ``wrapped'' Normal distribution as proposed in \cite{Nagano2019Wrapped}.
%
%In both these approaches, the variational distribution $\bold{q}$ is smooth, however the posterior probability is not guaranteed to be continuous let alone differentiable as the topology changes.
%However this difficulty is partly overcome since the ELBO is not a point estimate, but is an expectation taken from $k$ samples.
%Any discontinuities in $p(z,D)$ are smoothed through sampling under $q$.
%
%To optimise the elbo we use stochastic gradient ascent implemented in PyTorch.
%The autograd functionality of PyTorch automatically computes the gradient of the elbo in order to stochastically optimise with gradients.

%The final log likelihood from BEAST was -6280 (similar to it's log posterior of -6271). In comparison, Dodonaphy MCMC gave -6280, which is a close match.
%However, figure~\ref{fig:stat_cmp}a illustrates that samples from Dodonaphy's MCMC always gave one incorrect split.
%This could indicate that the MCMC chain is stuck in a local optima and that techniques like using Metropolis-coupled MCMC to escape this optima should be employed.
%
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/compare-SF}%
%\includegraphics[width=.333\linewidth]{fig/cmp_tree_length}%
%\includegraphics[width=.333\linewidth]{fig/cmp_treeness}
%\includegraphics[width=.25\linewidth]{fig/cmp_b1}%
%\includegraphics[width=.25\linewidth]{fig/cmp_b2}%
%\includegraphics[width=.25\linewidth]{fig/cmp_b3}%
%\includegraphics[width=.25\linewidth]{fig/cmp_b4}
%\caption{Comparison of tree statistics between Beast and Dodonaphy: (a) split posterior probability (b) total branch length (c) ``treeness'' (signal/signal+noise) (d-g) branch lengths (possibly not same branch??).}
%\label{fig:stat_cmp}
%\end{center}
%\end{figure}
%
%Figure~\ref{fig:locations} shows a kernel density estimate for each node location at various stages.
%Comparing figures~\ref{fig:locations}a and b reveals that the posterior surface of nodes can drift over the simulation.
%Indeed tree reconstructions are invariant to isometries on the embedded points since they are only based on their relative distances.
%Additionally, tree reconstructions won't change as a tip node moves along the surface with constant distance to its nearest neighbour (provided the tree doesn't change topology).
%The embedded posterior surfaces generated from samples $\bold{X}_{i}$ shouldn't be considered static, but transient over the simulation.
%Nonetheless, these degrees of freedom should not affect the result of the MCMC.
%
%These densities are clearly not normally distributed.
%The feasibility of VI rests on how well a simple distribution $q$ can approximate these densities.
%Boosting to a mixture model may increase the ability for VI to represent these embeddings.
	
%\subsection{VI}
%The traces in figure~\ref{fig:elbos} illustrate that the elbos has a strange start, getting worse for a few hundred epochs, but then increases steadily but probably hasn’t converged after the 1e4 epochs.
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/mcmc_mst_locations.png}%
%\includegraphics[width=.333\linewidth]{fig/mcmc_geo_locations.png}%
%\includegraphics[width=.333\linewidth]{fig/mcmc_tri_locations.png}
%\includegraphics[width=.333\linewidth]{fig/vi_simple_mst_locations.png}%
%\includegraphics[width=.333\linewidth]{fig/vi_simple_geodesics_locations.png}%
%\includegraphics[width=.333\linewidth]{fig/vi_simple_incentre_locations.png}
%\caption{Density estimates of the node locations from Dodonaphy's MCMC projected into $\mathbb{R}^{2}$. MCMC: using (a) mst, (b) geodesics, (c) incentres . 
%Variational densities after learning for $10^{3}$ epochs in $\mathbb{P}^{2}$ using (d) mst, (e) geodesics, (f) incentres.
%}
%\label{fig:elbos}
%\end{center}
%\end{figure}
%
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_geodesics.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_incentre.png}
%\caption{ELBO trace from VI on six taxa using the ``simple'' embedding with (a) MST, (b) geodesics, (d) incentres. All with $k=10$ ELBO samples and a learning rate of $10^{-3}$.}
%\label{fig:elbo_method}
%\end{center}
%\end{figure}
%
%\begin{figure}[htbp]
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k1.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k5.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k10.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k20.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k50.png}
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr4_k1.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr4_k5.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr4_k10.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr4_k20.png}
%\caption{Effect of number of ELBO samples $k=\{1, 5, 10, 20, 50\}$.
%Top row with learning rate $10^{-3}$ and bottom row $10^{4}$.
%All using a ``simple'' embedding.}
%\label{fig:k_samples}
%\end{figure}
%
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr2.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr3.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr4.png}
%\caption{Effect of learning rate $\{10^{-2}, 10^{-3}, 10^{-4}\}$ with fixed $k=10$ under MST using ``simple'' embedding.}
%\label{fig:lr}
%\end{center}
%\end{figure}
%
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr2.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr3.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr4.png}
%\caption{Effect of learning rate $\{10^{-2}, 10^{-3}, 10^{-4}\}$ with fixed $k=10$ under MST using ``simple'' embedding.}
%\label{fig:lr}
%\end{center}
%\end{figure}

\nocite{*}
\bibliographystyle{apalike}
\bibliography{notes}

\clearpage
\section{Appendix}
\subsection{Numerical stability in Hyperbolic space}
As either $||x||_{2}^{2}\to 1$ or $||y||_{2}^{2} \to 1$, eq.~\ref{eq:poin_dist} can become a numerically unstable way to compute distances.
Since the poincare ball is a stereographic projection of the hyperboloid, an equivalent metric comes from projecting of $x$ and $y$ into the hyperboloid model of $\mathbb{H}^{d}$ and using its metric.
The hyperboloid model is the sheet inside $\bold{x}\in\mathbb{R}^{d+1}$ such that $x_{0}^{2} - \sum_{i} x_{i}^{2} = 1$.
It has metric $d_{hyp}(\bold{x}, \bold{y}) = \text{acosh}(-\langle \bold{x}, \bold{y} \rangle)$, where the Lorentz inner product of $\bold{x}$ and $\bold{y}$ is:
\begin{equation*}
	\langle \bold{x}, \bold{y} \rangle = -x_{0}y_{0} + \sum_{i>0} x_{i}y_{i}
\end{equation*}
The stereographic projection onto the hyperboloid $\phi:\mathbb{P}^{d} \to \mathbb{H}^{d}$ takes a point to $\phi(\bold{x}) = \big(\frac{(1+||x||_{2})}{(1-||x||_{2})}, \frac{2 \bold{x}}{(1- ||x||_{2}}\big)$.
Thus, for $\bold{x}, \bold{y} \in \mathbb{P}_{d}$ we can use $d(\bold{x}, \bold{y}) = d_{hyp}(\psi(\bold{x}), \psi(\bold{y}))$.

% define \Phi inverse explicitly in

\subsection{Normalising Jacobian}
Normalising the leaf positions to radius $r$ by $n_{r}(x) = r \bold{x}/||\bold{x}||$ has Jacobian
\be
\frac{\partial n_{r}(\bold{x})}{\partial \bold{x}} = r \frac{\partial n_{1}(\bold{x})}{\partial \bold{x}} = 
\dfrac{r}{||x||} \Big( I - \dfrac{x \otimes x}{||x||^{2}} \Big)
\ee

\paragraph{Wrapping Method}
We propose a node's new location from a Gaussian by projecting the point from the Poincare ball into $\Psi: \mathbb{P}^{d} \to \mathbb{R}^{d}$.
First project onto the hyperboloid model in $\mathbb{R}^{d+1}$ using $\text{proj}^{-1}(\bold{x})$ (see appendix), then projecting onto the tangent plane of $\bold{x}=0$ using the $d$-dimensional vector $u=(0, 1, 1, 1,...)$: $\Psi(\bold{x}_{i}) = \text{proj}^{-1}(\bold{x}_{i}) \cdot u $.
A new point is drawn from a Gaussian located at $\Psi(\bold{x}_{i})$ with given standard deviation, before being projected back to the Poincare ball with $\Phi^{-1}$.

The determinate of the Jacobian of the projection $\text{proj}_{\mu}: \text{exp}_{\mu} \circ \text{PT}{\mu_{0} \to \mu}$:
\be
\Big| \frac{\text{proj}_{\mu} (\bold{x})} {\partial \bold{x}} \Big| = \Big(\dfrac{\sinh(||x||)}{||x||}\Big)^{n-1}.
\ee
The Jacobian of stereographic projection projection back into the poincare ball $
% Double check this
\phi^{-1}$ is:
\bee
\frac{\partial \phi^{-1}(\bold{x})} {\partial \bold{x}} = \begin{cases}
\dfrac{1}{1+x_{0}} &\text{ if } j = i+1\\
\dfrac{-x_{i+1}}{(1+x_{0})^{2}} &\text{ if } j = 0\\
0 &\text{ otherwise}\\
\end{cases}
\eee
Since this projection is from $\mathbb{R}^{d+1}$ to $\mathbb{R}^{d}$, the Jacobian matrix is not square and so we  use the $d$-dimensional Jacobian $|J_{\phi^{-1}}(\bold{x})| = (\text{det} (J_{\phi^{-1}} (\bold{x}) J_{\phi^{-1}} (\bold{x})^{T}))^{1/2}$. The determinant of the Jacobian simplifies to:
\be
\Big|\frac{\partial {\phi^{-1}}(\bold{x})}{\partial \bold{x}}\Big| = \dfrac{(1+x_{0})^{2} + \sum_{i=1}^{d} x_{i}^{2}}{(1+x_{0})^{2(d+1)}}.
\ee

\paragraph{Discontinuities}
Note that the likelihood function is discontinuous as the topology changes.
This means the optima found may be only locally optimal and may depend on the starting location.
The initialisation by Hydra aims to mitigate this effect.

\paragraph{Brute force MST}
The figure below shows a grid search of the posterior landscape under a MST protocol.
We could do a similar thing for neighbour joining now that we're primarily using it.
\begin{figure}[htbp]
    \begin{center}
    \includegraphics[width=.33\linewidth]{fig/node6_posterior}%
    \includegraphics[width=.33\linewidth]{fig/node8_posterior}%
    \includegraphics[width=.33\linewidth]{fig/node9_posterior}
    \end{center}
    \caption{Fix all node positions but one (a) node 6, (b) node 8 and (c) node 9. Move this one node throughout the Poincaré disk and plot the tree posterior by placing the node at that point.}
    \end{figure}
    

\end{document}

We modify Prim's algorithm to add edges one by one, but only if the edges are valid.
An edge $e_{ij}$ from vertex $i$ to $j$ is valid provided: a) vertex $v_{j}$ has not been visited, b) $e_{ij}$ doesn't add a second edge to a leaf node, and c) internal vertices have up to three neighbours, of which at least one must be internal.
 
Unlike Prim's algorithm, which can start at an arbitrary vertex, algorithm~\ref{alg:bmst} starts from the shortest edge between a leaf and a internal vertex.
If an arbitrary vertex were used, say a leaf node, the minimum binary spanning tree may not form an edge to its nearest neighbour.
This is because it 

\begin{algorithm} \label{alg:bmst}
\caption{Binary Minimum Spanning Tree}
Add the shortest leaf-int edge to queue\;
\While{\textnormal{queue} $\neq \emptyset$ and we haven't visited all vertices}{
$e_{ij} \gets$ pop the shortest edge in queue\;
\If{$e_{ij}$ is valid} {
add $e_{ij}$ to the spanning tree\;
add all edges from the vertex $v_{j}$ to queue\;
mark vertex $v_{j}$ as visited
} 
}
Not every tree may be accessible under this MST protocol and in this sense, we have a variational distribution over trees.
Given a connection protocol, is there a limit to the trees that can be generated?
\end{algorithm}