\documentclass[11pt, twocolumn]{article}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{epstopdf}
\usepackage{hyperref}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\newtheorem{theorem}{Theorem}
\newcommand{\be}{\begin{equation*}}
\newcommand{\ee}{\end{equation*}}
\newcommand{\bee}{\begin{eqnarray*}}
\newcommand{\eee}{\end{eqnarray*}}

\title{Variational Phylogenetic Inference in Hyperbolic Space}

\begin{document}
\maketitle

\section{Abstract}
\textbf{
Hyperbolic spaces allow quality embeddings of nested data structures, such as trees.
Recent efforts to embed phylogenetic trees using distance-based optimisation demonstrate promising results for embedding single trees.
However, expressing uncertainty in phylogenetics involves a probability distribution over a super-exponential number of trees.
Here, using low dimensional hyperbolic tree embeddings, we explore the Bayesian posterior distribution of trees in a continuous manner.
First, we empirically demonstrate that a posterior surface can be well approximated with tree embeddings using an MCMC in the embedding space.
We also evaluate the potential of variational inference in the embedding space.
}

\section{Introduction}
Bayesian phylogenetics has struggled when provided with many taxa.
The super-exponential number of combinations of tree topologies is largely responsible for this;
even a modest one hundred taxa provides more tree topologies than the number of atoms in the universe, clearly marginalising over these topologies is intractable.
Markov Chain Monte Carlo (MCMC) has become a routine way forwards in this high dimensional space.
However, its computational performance is lacklustre compared to online viral outbreaks, calling for faster methods.

One possibility is a variational approximation to the posterior, which minimises the divergence between a chosen approximating function and the posterior.
This technique avoids the computational burden of computing the marginal likelihood of the data by instead optimising a lower bound for the evidence (ELBO).
Furthermore, it is possible to encode multiple tree topologies in a continuous manner using embeddings.
Embedding trees into Riemannian manifolds can enable the efficiencies gains from gradient based optimisation, even as tree topologies change.

A successful embedding requires an isometry between distances in trees and distances in the embedding space. 
Previous work has shown that ultrametric trees always have an isometric embedding in Euclidean space~\cite{pavoine2005measuring}.
Furthermore, there is an isometric embedding in Euclidean space for any rooted phylogenetic tree by taking the square-root of distances between leaves on the tree~\cite{devienne2011euclidean}.
However, Euclidean embeddings generally have a minimum embedding dimension~\cite{deza1997geometry}, trading one high-dimensional problem for another.
Spurred on by quality embeddings of nested data structures (such as trees) in low dimensions~\cite{sala2018representation}, several recent works turn to hyperbolic space for embeddings~\cite{chami2020trees, wilson2021learning, matsumoto2020novel, lee2021deep}.
However, none of these works consider the problem of Bayesian inference in hyperbolic space.

In this work, we embed points onto a sphere $S^{d-1}$ in hyperbolic space $\mathbb{H}^d$, with each point corrresponding to a genetic sequence.
Initially points are embedded according to their pairwise genetic distance using an approximate strain minimisation algorithm.
Each point is equipped with a variational distribution on the sphere.
To sample a tree, we draw one point from each distribution and form the neighbour joining tree from their pairwise distance in hyperbolic space.
We then optimise these distributions by maximising the ELBO.


\section{Trees in Hyperbolic Space}
\subsection{Hyperbolic Space}
The most common model of Hyperbolic space is the Poincaré ball $\mathbb{P}^{d} = \{x\in \mathbb{R}^{d} \,:\, ||x||<1\}$ contain points in a unit ball equipped with the metric
\begin{equation*}\label{eq:poin_dist}
    d(x,y) = \textnormal{arccosh}\Big(1 + 2 \frac{||x-y||_{2}^{2}}{ (1-||x||_{2}^{2}) (1-||y||_{2}^{2})}\Big),
\end{equation*}
where $||x||_{2}$ is the $l^{2}$-norm in $\mathbb{R}^{d}$.
However, as either $||x||_{2}^{2}\to 1$ or $||y||_{2}^{2} \to 1$, eq.~\ref{eq:poin_dist} can become a numerically unstable way to compute distances~\cite{sala2018representation}.
Fortunately, the poincare ball is a stereographic projection of the hyperboloid model, so an equivalent metric comes from projecting of $x$ and $y$ into the hyperboloid model of $\mathbb{H}^{d}$ and using its metric.
The stereographic projection $\phi:\mathbb{P}^{d} \to \mathbb{H}^{d}$ takes a point to
\begin{equation}
    \phi(\bold{x}) = \Big(\frac{(1+||x||_{2})}{(1-||x||_{2})}, \frac{2 \bold{x}}{(1- ||x||_{2}}\Big).    
\end{equation}

Then the hyperboloid model $\mathbb{H}^d$ is the sheet inside $\bold{x}\in\mathbb{R}^{d+1}$ such that $x_{0}^{2} - \sum_{i} x_{i}^{2} = 1$.
It has metric
\begin{equation}
    d_{hyp}(\bold{x}, \bold{y}) = \text{acosh}(-\langle \bold{x}, \bold{y} \rangle),
\end{equation}
where the Lorentz inner product of $\bold{x}$ and $\bold{y}$ is:
\begin{equation}
	\langle \bold{x}, \bold{y} \rangle = -x_{0}y_{0} + \sum_{i>0} x_{i}y_{i}
\end{equation}
Thus, for $\bold{x}, \bold{y} \in \mathbb{P}^{d}$ we can use that $d(\bold{x}, \bold{y}) = d_{hyp}(\phi(\bold{x}), \phi(\bold{y}))$.
Because of their connection we use these two models somewhat interchangeably, modelling points in the Poincaré ball, but computing their distances on the hyperboloid.

\subsection{Sampling in Hyperbolic Space}
Given a point in hyperbolic space, we want to sample nearby points according to a distribution.
To do this, we project points from the Poincaré ball into $\mathbb{R}^d$ and sample distributions there before projecting back into $\mathbb{P}^d$.
This enables sampling to use common and efficient sampling techniques in $\mathbb{R}^d$, here we choose a multivariate normal distribution.
We consider two possible homeomorphisms $\mathbb{H}^n \to \mathbb{R}^d$: a method of wrapping Euclidean vectors onto the hyperboloid and a homeomorphism from the unit ball into $\mathbb{R}^n$.

\subsubsection{Wrapping Distributions}
This recently developed method by~\cite{nagano2019wrapped} centres around the projection of a vector $\text{proj}(\bold{x})$ from $\mathbb{R}^d$ onto the hyperboloid model, which we then take into the Poincaré ball.
The projection is the composition of two fuctions, firstly the vector is mapped via the parallel transport function along the hyperboloid in $\mathbb{R}^{d+1}$ to a desired location $\mu_0$.
Then the exponential map wraps the vector in $\mathbb{R}^{d+1}$ on the the hypderboloid $\mathbb{H}^d$:
\begin{equation}
    \text{proj}_{\mu}: \text{exp}_{\mu} \circ \text{PT}{\mu_{0} \to \mu}.
\end{equation}
TODO: improve notation ===
Further details are provided in the appendix.

To project a point from the Poincaré ball to Euclidean space, we first take the inverse stereographic projection from $\mathbb{P}^d$ to $\mathbb{H}^d$ then project ``downwards'' onto the tangent plane of $\bold{x}=0$ using the $d$-dimensional vector $u=(0, 1, 1,...): \mu = \phi^{-1}(\bold{x}_{i}) \cdot u $.
We may then sample a vector $v$ using any distribution in $\mathbb{R}^d$, such as a Gaussian $G(\mu, \Sigma)$, before projecting back to the Poincare ball with $\phi \circ \text{proj}_{\mu)}(v)$.

The determinate of the Jacobian of the projection $\text{proj}_{\mu}(x)$:
\be
\Big| \frac{\text{proj}_{\mu} (\bold{x})} {\partial \bold{x}} \Big| = \Big(\dfrac{\sinh(||x||)}{||x||}\Big)^{n-1}.
\ee
The Jacobian of stereographic projection projection back into the poincare ball $\phi^{-1}$ is:
\bee
\frac{\partial \phi^{-1}(\bold{x})} {\partial \bold{x}} = \begin{cases}
\dfrac{1}{1+x_{0}} &\text{ if } j = i+1\\
\dfrac{-x_{i+1}}{(1+x_{0})^{2}} &\text{ if } j = 0\\
0 &\text{ otherwise}\\
\end{cases}
\eee
Since this projection is from $\mathbb{R}^{d+1}$ to $\mathbb{R}^{d}$, the Jacobian matrix is not square and so we  use the $d$-dimensional Jacobian $|J_{\phi^{-1}}(\bold{x})| = (\text{det} (J_{\phi^{-1}} (\bold{x}) J_{\phi^{-1}} (\bold{x})^{T}))^{1/2}$, which simplifies to:
\be
\Big|\frac{\partial {\phi^{-1}}(\bold{x})}{\partial \bold{x}}\Big| = \dfrac{(1+x_{0})^{2} + \sum_{i=1}^{d} x_{i}^{2}}{(1+x_{0})^{2(d+1)}}.
\ee

\subsubsection{A simple homeomorphism}
Alternatively, we also consider the following embedding function $g: \mathbb{R}^{d} \to \mathbb{P}^{d}$:
\be
g(\bold{x}) = \dfrac{\bold{x}}{1+||\bold{x}||_{2}}
\ee
with gradient
\be
\frac{\partial g(\bold{x})}{\partial \bold{x}} = 
\dfrac{1}{1+||\bold{x}||_{2}} \Big( I - \dfrac{\bold{x} \otimes \bold{x}}{(1+||\bold{x}||_{2})||\bold{x}||_{2}} \Big).
\ee
Whilst this mapping does not preserve distances it has fewer computational steps.

\subsubsection{Forming a Tree}
From a set of leaf locations $x$ in the Poincaré ball $x_i \in \mathbb{P}^d$ we seek to construct a tree.
One way to form a tree is to use the hyperbolic pairwise distances between leaves and construct the neighbour joining (NJ) tree.
Neighbour joining ===

Recently, \cite{chami2020trees} ===

\cite{sarkar2012low} find a construction  to embed any given tree into the hyperbolic plane $\mathbb{H}^{2}$ with arbitrary low distortion.

We hypothesise that ultrametric trees can be embedded into hyperbolic space with their leaves lying on a sphere.

\section{MCMC}
Since each set of node embeddings corresponds to a tree which has a well defined likelihood and prior probability, MCMC can proceed with the standard Metropolis-Hastings algorithm.
The set of nodes are initialised to locations $x^{0}$.
For each node $x_{i}^t$ at step $t$, a new location is proposed and accepted or rejected according to the Metropolis-Hastings algorithm, giving the next iteration $x_i^{t+1}$.
Node locations are proposed using their projection into $\mathbb{R}^d$, a Gaussian sample is taken before taking its inverse projection back into $\mathbb{P}^d$.
For simplicity, leaves are restricted to a sphere by normalising their radius to a single value.
That is, each proposal is a Gaussian in $\mathbb{R}^d$, but we then scale it to have the same radius as the first leaf.

% We must show that the Markov Chains are ergodic, that is, as $m \to \infty$ the distribution $p(\bold{X}_{m}|\bold{X}_{0})$ converges to the posterior distribution regardless of the choice of $p(\bold{X}_{0})$.
% If the chain is not ergodic, then the choice of proposal partitions the state space into subsets which cannot be reached from each other.
% A point set $\bold{X}_{m} \in \mathbb{R}^{d}$ can be reached via our normal distribution proposal, however it remains to be seen if this embedding spans all trees.
% We must know theorem~\ref{thm:accessible}.
% % https://www.robots.ox.ac.uk/~fwood/teaching/C19_hilary_2014_2015/mcmc.pdf

\section{Comparison to MrBayes}
A posterior was approximated for a 17 taxa set using Dodonaphy's MCMC and its VI before being compared to MrBayes.
A tree was simulated using a birth (rate $2$) death (rate $.5$) model and a sequence alignment was generated from this tree under the JC69 model of genetic evolution.
The pairwise patristic distances were computed between the tips on the simulated tree.
The tips were initialised in the Poincare ball using hydra.
Then the internal nodes were randomly placed in $\mathbb{P}^{d}$ with uniform directional and radius from a scaled Beta distribution $ r \sim s \times \text{Beta}(a=2, \beta=5)$ using scales $s \in [0, 2*\min(d(0, \bold{x}_{i})]$, where $i$ only includes the tip nodes.
Internal node locations were sampled $10^{4}$ times and the initialisation with the highest tree likelihood was selected.
===

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=\linewidth]{fig/t17_trace_post}
\end{center}
\caption{Effect of embedding dimension on MCMC trace. (a) MCMC trace plots of the posterior using the simple embedding method connecting leaves using neighbour joining.
(b) Kernel density estimates of the posterior for the last 200 tree in figure (a).}
\end{figure}

\begin{figure}[htbp] \label{fig:splits}
    \begin{center}
        \includegraphics*[width=\linewidth]{fig/t17_splits_mcmc}
    \end{center}
    \caption{Comparison of splits on a tree with 17 taxa.
    The two solid lines indicate the posterior frequency of each split (indexed from zero) in two MrBayes runs.
    Markers represent the posterior frequency of these splits using MCMC in hyperbolic space.}
    \end{figure}

\subsection{Embedding Dimension}
Every phylogenetic tree has a Euclidean embedding~\cite{devienne2011euclidean}, however the number of dimensions $m$ to be sure of an embedding required grows linearly with the number of taxa $S$~\cite{deza1997geometry}: $m=S-1$.
On the other hand, hyperbolic embeddings offer low dimensional approximate embeddings of distance spaces.
Figure~\ref{fig:map_dim} empirically demonstrates that the embedding quality saturates after only 4 or 5 dimensions for a tree with 17 taxa.

\begin{figure}[htbp] \label{fig:map_dim}
\begin{center}
    \includegraphics*[width=\linewidth]{fig/map_dim}
\end{center}
\caption{Improvement of the MAP estimate on a tree with 17 taxa with increasing dimension.}
\end{figure}

\section{Variational Inference}
In VI, the posterior surface (here a set of surfaces --- one for each embedded node $i$) is approximated by a variational distribution $q_{i}(z_{i})$, each parameterised by parameters $z_{i}$.
When the data $D$ is fixed, minimising the KL divergence to the posterior $\text{KL}({\bf q}(\phi)||p(T\big|D))$ is equivalent to maximising the evidence lower bound (ELBO):
\be
\text{ELBO}(q) = \mathbb{E}_{q}[\log(p(z,D))] - \mathbb{E}_{q}[\log(q(z))].
\ee
Working with the ELBO is computationally advantageous since it does not depend on the intractable marginal distribution of the data $p(D)$, which is involves integrate over all tree topologies and all branch lengths.

\subsection{Differentiable cost}
Tree topologies are discrete objects and thus gradient-based operations on embedded trees may be non-differentiable as the topology changes.
Finding a way to embed trees such that the likelihood is continuous as the topology changes could lead to local optimisation solutions.
However, finding such an embedding remains an open question.

To circumvent this issue, we optimise the likelihood not of a tree, but based on the pair-wise distance of every pair of nodes.
Given a set of leaf nodes in the embedding space $x \in \mathbb{H}^{d}$, the cost function employed is:
\begin{equation}
p(z, D) = \prod_{i \neq j} L(d_{ij}|D)))^{w_{ij}}  p(\textnormal{NJ}(d_{ij}))
\end{equation}

\begin{equation}
\log p(z, D) = \sum_{i \neq j} w_{ij} \log L(d_{ij}|D))) + \log p(\textnormal{NJ}(d_{ij}))
\end{equation}
\noindent where $L(d_{ij}|D)$ is the likelihood of the distance from $x_{i}$ to $x_{j}$ given the sequence alignment data $D$ and the weight $w_{ij}$ applied to each edge sums to one $\sum_{ij} w_{ij} = 1$.
Note that in most instances the likelihood the likelihood if less than one, so a larger weight gives less emphasis.

The prior probability $p(\textnormal{NJ}(d_{ij}))$ comes from forming a neighbour joining tree then computing a gamma Dirichlet prior on branch lengths and a uniform prior on topologies.
This definition does not requires a time reversible likelihood function, however we use a simple Jukes-Cantor model of evolution which gives $L(d_{ij})=L(d_{ji})$.

To better mimic the likelihood of a tree, we weight edges according to their embedding distance --- nodes that are closer neighbours according to the Q-criterion have less weight (more emphasis).
The Q-criterion is used to determine which edges are joined during neighbour joining (NJ): 
\be
Q_{ij} = (S-2)d_{ij} - \sum_{k}^{S}d(i, k) - \sum_{k}^{S}d(j, k).
\ee
In the neighbour joining algorithm, the most negative entry in $Q$ is selected to and these nodes are connected creating a new ancestor, before again selecting the most negative entry of $Q$.
Accordingly, the weight applied to each edge is constructed as the negative inverse of $Q$, then a normalised exponential (softmax) function is applied so the weights sum to one for each node:
\be
w_{ij} = \frac{\exp(n Q_{ij})}{\sum_{j} \exp(n Q_{ij})}.
\ee
The additional scaling by the current epoch number $n \in \{1,2,3...\}$ is put in as a cooling effect so that:
\be
\lim_{n\to\infty} w_{ij} = 
\begin{cases}
1 \qquad \textnormal{if } i = \argmin_{k}(d_{kj}) \\
0 \qquad \textnormal{otherwise}
\end{cases}
\ee

\subsection{Variational Distributions}
We approximated the posterior surface for each leaf node using a log-normal distribution $\textnormal{LogNormal}(x_{i}, \Sigma_{i})$ in Euclidean space $x_{i} \in \mathbb{R}^{d}$ wrapped onto a sphere.
Specifically, points were drawn from $X \sim \textnormal{LogNormal}(x_{i}, \Sigma_{i})$ then normalised to a lie on a sphere $||X||_{2} = ||x||_{i}$, before being mapped into $\mathbb{H}^{d}$ using either the wrapping or ``simple'' embedding methods previously outlined.
We assume mean field inference where each $\Sigma_{i}$ is independent of $\Sigma_{j}$.
Further each $\Sigma_{i}$ is a $d \times d$ covariance matrix that we assume is diagonal.

\subsection{Implementation}
To optimise the ELBO we use stochastic gradient ascent implemented in PyTorch.
The autograd functionality of PyTorch automatically computes the gradient of the elbo in order to stochastically optimise with gradients.

\subsection{Sampling Trees}
At the end of the optimisation, trees must be sampled to approximate the posterior distribution on trees.
We sample trees by sampling one point from each variational distribution (one per leaf node) and connect them using neighbour joining.
Connecting them in this way does not yield the same ===

\begin{theorem}
Could the distribution optimised by cost $C$ equal the neighbour joining tree in the limit?
\end{theorem}


%The variational distribution $\bold{q}$ is smooth, however the posterior probability is not guaranteed to be continuous let alone differentiable as the tree topology changes.
%Evidently the posterior for each node is unlikely to be normally distributed.
%The feasibility of VI rests on how well a simple distribution $q$ can approximate these densities.

\begin{figure}[htbp] \label{fig:elbo_trace}
\begin{center}
    \includegraphics*[width=\linewidth]{fig/elbo_trace_simple_nj_lr1_k2_d5_lal}
\end{center}
\caption{ELBO tree on a tree with 17 taxa in five dimensions. The learning rate is $10^{-1}$ and there were $k=2$ importance samples for each ELBO calculation.}
\end{figure}

%The final log likelihood from BEAST was -6280 (similar to it's log posterior of -6271). In comparison, Dodonaphy MCMC gave -6280, which is a close match.
%However, figure~\ref{fig:stat_cmp}a illustrates that samples from Dodonaphy's MCMC always gave one incorrect split.
%This could indicate that the MCMC chain is stuck in a local optima and that techniques like using Metropolis-coupled MCMC to escape this optima should be employed.
%
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/compare-SF}%
%\includegraphics[width=.333\linewidth]{fig/cmp_tree_length}%
%\includegraphics[width=.333\linewidth]{fig/cmp_treeness}
%\includegraphics[width=.25\linewidth]{fig/cmp_b1}%
%\includegraphics[width=.25\linewidth]{fig/cmp_b2}%
%\includegraphics[width=.25\linewidth]{fig/cmp_b3}%
%\includegraphics[width=.25\linewidth]{fig/cmp_b4}
%\caption{Comparison of tree statistics between Beast and Dodonaphy: (a) split posterior probability (b) total branch length (c) ``treeness'' (signal/signal+noise) (d-g) branch lengths (possibly not same branch??).}
%\label{fig:stat_cmp}
%\end{center}
%\end{figure}
%
%Figure~\ref{fig:locations} shows a kernel density estimate for each node location at various stages.
%Comparing figures~\ref{fig:locations}a and b reveals that the posterior surface of nodes can drift over the simulation.
%Indeed tree reconstructions are invariant to isometries on the embedded points since they are only based on their relative distances.
%Additionally, tree reconstructions won't change as a tip node moves along the surface with constant distance to its nearest neighbour (provided the tree doesn't change topology).
%The embedded posterior surfaces generated from samples $\bold{X}_{i}$ shouldn't be considered static, but transient over the simulation.
%Nonetheless, these degrees of freedom should not affect the result of the MCMC.
%
%Boosting to a mixture model may increase the ability for VI to represent these embeddings.
	
%\subsection{VI}
%The traces in figure~\ref{fig:elbos} illustrate that the elbos has a strange start, getting worse for a few hundred epochs, but then increases steadily but probably hasn’t converged after the 1e4 epochs.
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/mcmc_mst_locations.png}%
%\includegraphics[width=.333\linewidth]{fig/mcmc_geo_locations.png}%
%\includegraphics[width=.333\linewidth]{fig/mcmc_tri_locations.png}
%\includegraphics[width=.333\linewidth]{fig/vi_simple_mst_locations.png}%
%\includegraphics[width=.333\linewidth]{fig/vi_simple_geodesics_locations.png}%
%\includegraphics[width=.333\linewidth]{fig/vi_simple_incentre_locations.png}
%\caption{Density estimates of the node locations from Dodonaphy's MCMC projected into $\mathbb{R}^{2}$. MCMC: using (a) mst, (b) geodesics, (c) incentres . 
%Variational densities after learning for $10^{3}$ epochs in $\mathbb{P}^{2}$ using (d) mst, (e) geodesics, (f) incentres.
%}
%\label{fig:elbos}
%\end{center}
%\end{figure}
%
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_geodesics.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_incentre.png}
%\caption{ELBO trace from VI on six taxa using the ``simple'' embedding with (a) MST, (b) geodesics, (d) incentres. All with $k=10$ ELBO samples and a learning rate of $10^{-3}$.}
%\label{fig:elbo_method}
%\end{center}
%\end{figure}
%
%\begin{figure}[htbp]
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k1.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k5.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k10.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k20.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr3_k50.png}
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr4_k1.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr4_k5.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr4_k10.png}%
%\includegraphics[width=.2\linewidth]{fig/elbo_trace_mst_lr4_k20.png}
%\caption{Effect of number of ELBO samples $k=\{1, 5, 10, 20, 50\}$.
%Top row with learning rate $10^{-3}$ and bottom row $10^{4}$.
%All using a ``simple'' embedding.}
%\label{fig:k_samples}
%\end{figure}
%
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr2.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr3.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr4.png}
%\caption{Effect of learning rate $\{10^{-2}, 10^{-3}, 10^{-4}\}$ with fixed $k=10$ under MST using ``simple'' embedding.}
%\label{fig:lr}
%\end{center}
%\end{figure}
%
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr2.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr3.png}%
%\includegraphics[width=.333\linewidth]{fig/elbo_trace_mst_lr4.png}
%\caption{Effect of learning rate $\{10^{-2}, 10^{-3}, 10^{-4}\}$ with fixed $k=10$ under MST using ``simple'' embedding.}
%\label{fig:lr}
%\end{center}
%\end{figure}

\section{Acknowledgement}
Computational facilities were provided by the UTS eResearch High Performance Computer Cluster.

\nocite{*}
\bibliographystyle{apalike}
\bibliography{notes}

\clearpage
\section{Thoughts}
\subsection{Embedding}
\begin{itemize}
\item I tried without the Matsumuto adjustment and the likelihood was a bit worse.
\end{itemize}

\subsection{MCMC}
\begin{itemize}
\item What type of tree rearrangements occur when topology changes?
\item Posterior surfance is smooth if topology doesn't change.
\item Remove isometric component of new vector. Want rotations and translations component
\item I tried using log-a-like function instead of likelihood \cite{wilson2021learning} (leaves constrained to a sphere). It gave terrible trees on a six taxa dataset - likelihood $\sim -8000$ (not $-2600$) and bad splits. I tried weighting the edges inversely with their length (shorter edges are weighted more) and the result was about the same.
\item Try on more taxa 20, 100, 200 using hpc
\item How easy is it to add taxa? Might only need pair-wise distance to a subset of other taxa.
\end{itemize}

\subsection{VI}
\begin{itemize}
\item Pytorch's optimisers don't converge for non-convex problems.
\item Posterior ``surface'' is non-convex and not continuous...
\item What if we learn the curvature?
\end{itemize}

\subsection{Full rank}
Intuitively, nodes that are close together should be a bit correlated. However, it doesn't seem to improve things much. In the off-diagonals in the covariance matrix are initialised to zero, the ELBO gets much higher faster compared to if the off-diagonal terms have a non-zero covariance. That said, I haven't run simulations long enough to be sure, only $1000$ epochs with a small learning rate of $0.01$.

\subsection{Distance-based}
Could we adopt an approach like Wilson and Chami, where only the distributions of the embedded points are optimised based on their pair-wise distances.
Only then do we infer a tree.
The advantage of this is that the cost function is differentiable and more in line with what other people in the ML community do.
However, this isn't actually modelling the Bayesian Posterior, just a proxy for it.


\clearpage
\section{Appendix}

% define \Phi inverse explicitly in

\subsection{Normalising Jacobian}
Normalising the leaf positions to radius $r$ by $n_{r}(x) = r \bold{x}/||\bold{x}||$ has Jacobian
\be
\frac{\partial n_{r}(\bold{x})}{\partial \bold{x}} = r \frac{\partial n_{1}(\bold{x})}{\partial \bold{x}} = 
\dfrac{r}{||x||} \Big( I - \dfrac{x \otimes x}{||x||^{2}} \Big)
\ee

\subsection{Details of wrapping}
TODO ===

\paragraph{Discontinuities}
Note that the likelihood function is discontinuous as the topology changes.
This means the optima found may be only locally optimal and may depend on the starting location.
The initialisation by Hydra aims to mitigate this effect.

% \paragraph{Euclidean Embedding Dimension}
% We simulated a number of birth-death trees to compute their minumum embedding dimension.
% Given a pairwise distance matrix $d$, proposition 6.2.4 in~\cite{deza1997geometry} gives the minimum embedding dimension in Euclidean space is the rank of the matrix
% \begin{equation} \label{eq:euclid_dim}
%     p_{ij} = \frac{1}{2}(d_{i,N}^2 + d_{j,N}^2 - d_{i,j}^2).
% \end{equation}
% Trees were simulated using a birth rate of $2$ and a death rate of $1/2$.
% From the pairwise patristic distance on each tree we computed the minimum embedding dimension according to eq.~\ref{eq:euclid_dim}.
% Figure~\ref{fig:euclid_dim} illustrates how increasing the number of taxa linearly increases the minimum embedding dimension.
% \begin{figure}[htbp]
% \begin{center}
% \includegraphics[width=\linewidth]{fig/euclid_dim.png}
% \end{center}
% \caption{The minimum Euclidean embedding dimension of simulated birth-death trees against the size of their taxon set.
% The fitted line is $y=x-2$.}
% \label{fig:euclid_dim}
% \end{figure}

\paragraph{Brute force MST}
The figure below shows a grid search of the posterior landscape under a MST protocol.
We could do a similar thing for neighbour joining now that we're primarily using it.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=.33\linewidth]{fig/node6_posterior}%
\includegraphics[width=.33\linewidth]{fig/node8_posterior}%
\includegraphics[width=.33\linewidth]{fig/node9_posterior}
\end{center}
\caption{Fix all node positions but one (a) node 6, (b) node 8 and (c) node 9. Move this one node throughout the Poincaré disk and plot the tree posterior by placing the node at that point.}
\end{figure}
    

\end{document}
\section{MST}
We modify Prim's algorithm to add edges one by one, but only if the edges are valid.
An edge $e_{ij}$ from vertex $i$ to $j$ is valid provided: a) vertex $v_{j}$ has not been visited, b) $e_{ij}$ doesn't add a second edge to a leaf node, and c) internal vertices have up to three neighbours, of which at least one must be internal.
 
Unlike Prim's algorithm, which can start at an arbitrary vertex, algorithm~\ref{alg:bmst} starts from the shortest edge between a leaf and a internal vertex.
If an arbitrary vertex were used, say a leaf node, the minimum binary spanning tree may not form an edge to its nearest neighbour.
This is because it 

\begin{algorithm} \label{alg:bmst}
\caption{Binary Minimum Spanning Tree}
Add the shortest leaf-int edge to queue\;
\While{\textnormal{queue} $\neq \emptyset$ and we haven't visited all vertices}{
$e_{ij} \gets$ pop the shortest edge in queue\;
\If{$e_{ij}$ is valid} {
add $e_{ij}$ to the spanning tree\;
add all edges from the vertex $v_{j}$ to queue\;
mark vertex $v_{j}$ as visited
} 
}
Not every tree may be accessible under this MST protocol and in this sense, we have a variational distribution over trees.
Given a connection protocol, is there a limit to the trees that can be generated?
\end{algorithm}

% For an unrooted tree with $S$ taxa, there will be $m=2S - 2$ nodes locations $\bold{X} \in(\mathbb{P}^{d})^{m}$, where $\bold{X} = \{\bold{x}_{i}: i=1, ..., m\}$.
% Embedded nodes are connected to form a minimum spanning tree (MST) protocol that ensures internal nodes have three neighbours and tip nodes have one neighbour.
